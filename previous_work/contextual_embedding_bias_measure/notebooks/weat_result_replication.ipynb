{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from bert_utils import Config, BertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    max_seq_len=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BertPreprocessor(config.model_type, config.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertConfig, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(config.model_type)\n",
    "model.eval() # Important! Disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(sentence: str) -> np.ndarray:\n",
    "    return model(processor.to_bert_model_input(sentence))[0, :, :].cpu().detach().numpy()\n",
    "\n",
    "def softmax(arr, axis=1):\n",
    "    e = np.exp(arr)\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_mask_fill_logits(sentence: str, words: Iterable[str],\n",
    "                         use_last_mask=False, apply_softmax=True) -> Dict[str, float]:\n",
    "    mask_i = processor.get_index(sentence, \"[MASK]\", last=use_last_mask, accept_wordpiece=True)\n",
    "    logits = defaultdict(list)\n",
    "    out_logits = get_logits(sentence)\n",
    "    if apply_softmax: \n",
    "        out_logits = softmax(out_logits)\n",
    "    return {w: out_logits[mask_i, processor.token_to_index(w, accept_wordpiece=True)] for w in words}\n",
    "\n",
    "def bias_score(sentence: str, gender_words: Iterable[Iterable[str]], \n",
    "               word: str, gender_comes_first=True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Input a sentence of the form \"GGG is XXX\"\n",
    "    XXX is a placeholder for the target word\n",
    "    GGG is a placeholder for the gendered words (the subject)\n",
    "    We will predict the bias when filling in the gendered words and \n",
    "    filling in the target word.\n",
    "    \n",
    "    gender_comes_first: whether GGG comes before XXX (TODO: better way of handling this?)\n",
    "    \"\"\"\n",
    "    # probability of filling [MASK] with \"he\" vs. \"she\" when target is \"programmer\"\n",
    "    mwords, fwords = gender_words\n",
    "    all_words = mwords + fwords\n",
    "    subject_fill_logits = get_mask_fill_logits(\n",
    "        sentence.replace(\"XXX\", word).replace(\"GGG\", \"[MASK]\"), \n",
    "        all_words, use_last_mask=not gender_comes_first,\n",
    "    )\n",
    "    subject_fill_bias = np.log(sum(subject_fill_logits[mw] for mw in mwords)) - \\\n",
    "                        np.log(sum(subject_fill_logits[fw] for fw in fwords))\n",
    "    # male words are simply more likely than female words\n",
    "    # correct for this by masking the target word and measuring the prior probabilities\n",
    "    subject_fill_prior_logits = get_mask_fill_logits(\n",
    "        sentence.replace(\"XXX\", \"[MASK]\").replace(\"GGG\", \"[MASK]\"), \n",
    "        all_words, use_last_mask=gender_comes_first,\n",
    "    )\n",
    "    subject_fill_bias_prior_correction = \\\n",
    "            np.log(sum(subject_fill_prior_logits[mw] for mw in mwords)) - \\\n",
    "            np.log(sum(subject_fill_prior_logits[fw] for fw in fwords))\n",
    "    \n",
    "    return {\n",
    "            \"stimulus\": word,\n",
    "            \"bias\": subject_fill_bias,\n",
    "            \"prior_correction\": subject_fill_bias_prior_correction,\n",
    "            \"bias_prior_corrected\": subject_fill_bias - subject_fill_bias_prior_correction,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flower': 0.0007418045, 'bug': 1.07483065e-05}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mask_fill_logits(\"the [MASK] is beautiful\", [\"flower\", \"bug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence: str, word: str):\n",
    "    idx = processor.get_index(sentence, word, accept_wordpiece=True)\n",
    "    outputs = None\n",
    "    with torch.no_grad():\n",
    "        sequence_output, _ = model.bert(processor.to_bert_model_input(sentence),\n",
    "                                        output_all_encoded_layers=False)\n",
    "        sequence_output.squeeze_(0)\n",
    "    return sequence_output.detach().cpu().numpy()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_size(df1, df2, k=\"bias_prior_corrected\"):\n",
    "    diff = (df1[k].mean() - df2[k].mean())\n",
    "    std_ = pd.concat([df1, df2], axis=0)[k].std() + 1e-8\n",
    "    return diff / std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_mc_perm_test(xs, ys, nmc=100000):\n",
    "    n, k = len(xs), 0\n",
    "    diff = np.abs(np.mean(xs) - np.mean(ys))\n",
    "    zs = np.concatenate([xs, ys])\n",
    "    for j in range(nmc):\n",
    "        np.random.shuffle(zs)\n",
    "        k += diff < np.abs(np.mean(zs[:n]) - np.mean(zs[n:]))\n",
    "    return k / nmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.06368981e-01,  2.22109944e-01, -7.18900710e-02,  1.70865208e-01,\n",
       "        3.62271130e-01, -1.20856173e-01, -6.16569072e-02,  7.87993014e-01,\n",
       "        3.73470277e-01, -2.52068788e-02,  2.59380415e-02, -1.49484515e+00,\n",
       "       -1.80714339e-01,  1.11428046e+00, -1.03053045e+00,  8.63718629e-01,\n",
       "        1.34956554e-01,  1.07128906e+00,  1.33132488e-01,  8.86740983e-01,\n",
       "       -1.61312252e-01, -5.93275666e-01,  7.23842531e-02, -5.57698727e-01,\n",
       "        1.10267997e+00, -5.62288702e-01, -2.21851468e-01,  8.34756434e-01,\n",
       "        4.88106251e-01, -9.58737545e-03,  2.16243431e-01,  1.58447415e-01,\n",
       "        5.18487573e-01,  2.14277059e-01,  1.41341776e-01, -2.22598836e-01,\n",
       "       -1.90466464e-01,  3.14576089e-01, -4.24953640e-01, -3.25795412e-01,\n",
       "       -7.41146207e-01, -1.04409921e+00,  6.84069693e-01,  4.49090391e-01,\n",
       "        2.56366640e-01, -7.32405841e-01, -9.85546291e-01,  3.55273962e-01,\n",
       "        9.63769078e-01,  3.46497685e-01,  5.14260270e-02,  5.61741829e-01,\n",
       "        2.53276557e-01, -1.06839582e-01,  4.62810844e-01,  5.47232985e-01,\n",
       "       -8.03264439e-01, -7.77305782e-01, -2.00483069e-01, -5.28368652e-01,\n",
       "       -4.77690130e-01,  8.90957415e-02,  3.79050702e-01, -1.00972623e-01,\n",
       "        9.57354382e-02, -8.97196531e-02, -1.48199782e-01,  1.25381127e-01,\n",
       "       -9.09168482e-01, -4.43255305e-01,  1.24072984e-01,  4.07963157e-01,\n",
       "        3.49193692e-01, -5.10466397e-02, -4.53874558e-01,  2.38604575e-01,\n",
       "       -3.83497477e-01,  2.26236850e-01, -2.38299608e-01, -4.38013464e-01,\n",
       "       -2.28099123e-01, -1.11407205e-01,  4.13440824e-01,  3.87609601e-01,\n",
       "       -9.92384851e-02,  8.58978555e-02, -9.76567388e-01,  4.05916125e-02,\n",
       "       -8.99848193e-02,  2.05641463e-01,  1.58096999e-01, -1.85252249e-01,\n",
       "       -5.00990033e-01,  2.39046738e-01, -1.34954974e-01, -4.15777504e-01,\n",
       "        7.87928179e-02,  4.71143514e-01, -7.26344436e-02,  2.40168869e-01,\n",
       "        8.60921502e-01, -8.16042796e-02, -2.44584799e-01,  4.08138663e-01,\n",
       "       -6.02751017e-01, -4.35080647e-01, -6.49738684e-02, -1.62538052e-01,\n",
       "        2.65623093e-01,  7.45046318e-01, -7.62215137e-01,  4.02591825e-01,\n",
       "       -4.47385967e-01,  1.19652681e-01, -1.14981556e+00,  1.24807298e+00,\n",
       "        5.31869054e-01, -4.88356024e-01,  1.46928906e-01, -2.39980787e-01,\n",
       "        6.72569454e-01, -3.89886498e-01,  3.02860647e-01,  4.00121957e-01,\n",
       "       -1.69996873e-01, -5.11887789e-01,  1.92454278e-01,  1.86521038e-01,\n",
       "       -8.37388277e-01,  1.05222076e-01, -4.64141965e-01,  4.67706025e-01,\n",
       "       -1.85250938e-01, -5.01711607e-01,  2.23010242e-01, -9.75935996e-01,\n",
       "       -1.94809794e-01, -2.00362623e-01, -7.91728795e-02,  4.77515608e-02,\n",
       "       -3.62029701e-01,  5.16069472e-01, -1.61559820e-01,  1.06315076e+00,\n",
       "       -8.03826526e-02,  6.59516990e-01, -7.46724010e-03,  8.29805657e-02,\n",
       "       -2.24850401e-01, -7.57497668e-01,  4.00907844e-01,  2.02943131e-01,\n",
       "       -6.59538507e-02, -1.73678249e-03, -2.10254133e-01,  3.24883103e-01,\n",
       "       -6.15544379e-01,  6.77369595e-01,  1.87589467e-01,  3.29709873e-02,\n",
       "       -3.59585851e-01, -7.53547251e-01,  1.65893599e-01,  4.97107536e-01,\n",
       "       -3.97464752e-01, -4.43160623e-01,  3.82502854e-01,  2.65457422e-01,\n",
       "       -7.97319859e-02,  7.20816851e-01, -8.93282592e-01, -5.99093139e-02,\n",
       "        2.71350294e-01,  6.12363108e-02,  9.22666311e-01, -2.92799324e-01,\n",
       "        2.65346080e-01, -8.09284627e-01, -3.07701796e-01,  6.68435514e-01,\n",
       "       -2.71839380e-01,  6.16510868e-01,  4.73261476e-01,  7.08366990e-01,\n",
       "       -8.79258588e-02, -4.17531103e-01,  1.00453734e+00,  5.55702150e-01,\n",
       "       -2.85637200e-01, -1.49904445e-01, -1.97960675e-01, -6.10363305e-01,\n",
       "       -2.67986149e-01,  2.12840855e-01, -1.73632026e-01, -9.85312238e-02,\n",
       "       -3.27346995e-02, -3.27063292e-01, -9.93387163e-01, -3.90170366e-01,\n",
       "        6.61131918e-01,  1.64618939e-01, -2.00865805e-01, -4.29663807e-01,\n",
       "       -5.69214582e-01, -1.22738808e-01,  5.86378455e-01, -4.87890542e-01,\n",
       "        5.74025631e-01,  3.13331276e-01, -2.59083629e-01, -9.16627422e-03,\n",
       "        9.99531269e-01, -3.05078536e-01,  4.39469755e-01,  4.63630967e-02,\n",
       "       -6.72843233e-02,  1.23664588e-02, -6.13554195e-03,  1.25280358e-02,\n",
       "        3.91279697e-01,  2.25989133e-01, -5.14249146e-01,  7.67559528e-01,\n",
       "       -5.49520493e-01,  4.78257298e-01,  6.96447790e-01, -4.86617565e-01,\n",
       "        2.84597397e-01, -7.60635063e-02,  2.09487885e-01,  3.72304261e-01,\n",
       "        8.20942044e-01, -2.82124788e-01, -2.64740199e-01,  3.12003285e-01,\n",
       "       -1.26166716e-01,  1.17298737e-02, -1.29803643e-01,  7.37189174e-01,\n",
       "        7.90813416e-02,  5.44379354e-01, -1.34594947e-01,  8.24180692e-02,\n",
       "        3.50829810e-01, -3.43899488e-01,  3.32312882e-01, -2.02917501e-01,\n",
       "        5.53840160e-01, -6.55832946e-01, -2.59503394e-01,  4.76768583e-01,\n",
       "        3.65735330e-02, -5.60238004e-01, -1.65224865e-01, -7.70688206e-02,\n",
       "        8.48134309e-02,  6.74858332e-01,  1.67155758e-01, -5.50796054e-02,\n",
       "        2.92653978e-01,  9.48139355e-02,  3.49618912e-01, -1.01741724e-01,\n",
       "       -5.75546384e-01, -9.16644856e-02, -4.70333934e-01,  1.39525563e-01,\n",
       "        1.03665918e-01, -8.79750922e-02, -3.27949107e-01,  1.46400169e-01,\n",
       "       -1.55136958e-01,  2.56914407e-01, -4.41621065e-01, -6.69948876e-01,\n",
       "       -5.78221858e-01, -2.89916396e-01, -4.90754358e-02, -5.57358712e-02,\n",
       "        3.50906163e-01,  2.79748857e-01, -7.24174023e-01, -1.26637459e-01,\n",
       "        4.46231127e-01, -4.43808049e-01,  2.34892964e-01,  6.79355264e-01,\n",
       "       -3.44262838e-01, -1.74710810e-01, -9.37994301e-01,  5.34851611e-01,\n",
       "       -4.74429876e-01,  9.28930044e-02,  2.20538184e-01, -2.23297417e-01,\n",
       "        4.19267535e-01,  4.41841558e-02,  9.44655120e-01, -2.93298215e-02,\n",
       "        5.16590357e-01,  3.71194452e-01,  6.15360737e-01, -5.15531898e-01,\n",
       "       -4.88995552e-01,  7.38495231e-01, -1.74699128e-01,  2.31962785e-01,\n",
       "       -4.53382158e+00,  1.38129652e-01, -3.99392061e-02,  2.39824895e-02,\n",
       "        4.55935448e-01,  1.36820510e-01, -1.29579276e-01, -1.26011580e-01,\n",
       "        1.06470764e-01,  1.37061700e-01,  7.21079648e-01, -1.68523654e-01,\n",
       "        9.27704275e-01,  1.15799606e-01,  3.16249192e-01, -4.17039931e-01,\n",
       "        3.86142761e-01, -1.19816017e+00, -3.30825984e-01,  1.68343082e-01,\n",
       "       -6.97976768e-01,  7.14323521e-02,  1.21875212e-01,  4.71082807e-01,\n",
       "        1.15364403e-01,  2.85468586e-02,  1.08732000e-01, -4.02184606e-01,\n",
       "       -7.08010554e-01,  7.19707072e-01, -6.40041232e-01, -3.56172532e-01,\n",
       "        6.06680095e-01,  7.74816155e-01, -7.36118615e-01, -4.47126061e-01,\n",
       "        4.02376354e-02, -6.86363935e-01,  2.88285494e-01,  2.14421347e-01,\n",
       "       -4.81394619e-01, -9.51110184e-01,  7.40530670e-01,  9.28562880e-01,\n",
       "        1.59917891e-01, -3.86909485e-01, -5.57639301e-01, -1.57299310e-01,\n",
       "       -2.53450006e-01,  7.66297400e-01,  7.25700617e-01, -5.26717603e-02,\n",
       "        4.22823519e-01, -4.83141333e-01, -7.36653209e-02, -2.71598190e-01,\n",
       "        2.12322444e-01,  5.62419415e-01, -3.76054704e-01, -1.32401258e-01,\n",
       "       -9.67185438e-01, -3.04849535e-01, -1.35320261e-01, -3.68716538e-01,\n",
       "       -5.14563501e-01, -6.20891988e-01, -5.71697831e-01, -3.99815649e-01,\n",
       "        1.85978636e-01,  5.12269378e-01,  4.00952309e-01, -3.20577383e-01,\n",
       "       -1.97420239e-01, -1.55722570e+00, -5.24126291e-01, -4.06745493e-01,\n",
       "       -4.88030851e-01,  2.44067430e-01,  1.13590643e-01, -3.55472788e-02,\n",
       "        4.02468592e-01, -6.37131870e-01,  8.35756242e-01,  4.04143155e-01,\n",
       "        6.41270220e-01, -7.00438917e-01, -1.38531476e-01, -4.34188306e-01,\n",
       "        8.99959952e-02,  8.98114890e-02,  1.78909421e-01,  4.16035801e-02,\n",
       "        1.30685121e-01,  5.43633640e-01,  1.61817521e-01, -3.23454291e-01,\n",
       "        4.83743668e-01,  3.85014445e-01,  7.64078557e-01, -8.34070206e-01,\n",
       "       -7.31380358e-02, -1.72004282e-01,  3.93361837e-01,  2.44497314e-01,\n",
       "       -8.41715485e-02, -4.31857288e-01, -1.02624631e+00,  1.33562073e-01,\n",
       "        3.45307171e-01,  1.86023384e-01,  1.26864409e+00, -3.96883667e-01,\n",
       "        7.20792055e-01, -5.74663401e-01,  7.64546245e-02,  7.88794979e-02,\n",
       "        4.09598053e-01,  2.75424331e-01,  8.58077765e-01, -2.31392816e-01,\n",
       "       -7.54590034e-01,  6.75302386e-01, -9.43761319e-02, -1.94428906e-01,\n",
       "        2.03639761e-01,  2.28710979e-01, -7.69647062e-01, -6.05421476e-02,\n",
       "       -9.27394450e-01,  2.02782229e-01,  7.57926106e-02, -4.03024375e-01,\n",
       "       -2.50724554e-01,  6.90565944e-01, -3.04469705e-01,  4.84251112e-01,\n",
       "       -1.05760849e+00, -1.22488630e+00, -2.68401951e-02,  4.93184984e-01,\n",
       "       -2.82255094e-02,  5.37833832e-02, -7.81419873e-01,  5.19675851e-01,\n",
       "        1.17235065e+00,  4.66052368e-02,  1.40038326e-01, -3.94703895e-01,\n",
       "       -4.01081920e-01, -4.00583804e-01, -3.29864919e-01, -3.89830291e-01,\n",
       "        8.50432664e-02, -1.00752425e+00, -6.19486570e-01, -1.16372538e+00,\n",
       "        2.22055390e-01,  2.31030703e-01, -5.49963295e-01, -7.19014406e-01,\n",
       "        2.97052890e-01,  2.51613081e-01, -1.85337409e-01, -1.48723513e-01,\n",
       "        5.02508759e-01,  1.38924599e-01,  3.66462260e-01, -2.06580952e-01,\n",
       "       -2.47316360e-01,  1.05458927e+00, -1.24157429e-01,  2.89026290e-01,\n",
       "       -6.68336093e-01, -2.57178098e-01, -1.54251039e-01,  4.66681868e-01,\n",
       "       -6.68328822e-01,  2.04154059e-01,  1.36733979e-01,  5.20241499e-01,\n",
       "       -1.22909248e-01, -2.04425767e-01, -3.31187576e-01,  2.44014785e-01,\n",
       "        6.06477559e-01, -2.26640776e-01,  1.73310623e-01, -1.15606987e+00,\n",
       "       -5.63660860e-01,  6.05199099e-01, -9.50557232e-01,  5.69975853e-01,\n",
       "       -8.86081696e-01, -5.27899265e-01, -7.94336200e-01, -6.86111569e-01,\n",
       "       -2.13280559e-01,  4.57363784e-01,  2.95509070e-01, -1.24294078e+00,\n",
       "        6.75270975e-01, -4.16886330e-01, -1.39369458e-01, -1.25983328e-01,\n",
       "       -1.08479790e-01, -4.27219778e-01,  6.90889657e-01,  3.16939205e-02,\n",
       "       -5.68817258e-01, -5.08291647e-02,  7.81225115e-02, -5.47761261e-01,\n",
       "       -1.06468332e+00, -8.24098825e-01, -4.01884429e-02,  4.05545086e-02,\n",
       "       -2.56478041e-01,  3.28599930e-01, -8.22212338e-01,  2.60284573e-01,\n",
       "        2.77458787e-01,  9.54261646e-02,  6.41825795e-03, -4.98912722e-01,\n",
       "       -2.92940527e-01, -7.07516670e-01, -1.52387515e-01,  4.57335383e-01,\n",
       "       -6.91831946e-01,  4.01380926e-01,  1.05152816e-01, -2.18253493e-01,\n",
       "        6.76671088e-01, -1.97770447e-01, -1.67898163e-02,  1.05948903e-01,\n",
       "       -7.11154640e-01, -3.23545009e-01,  3.90837610e-01,  4.15822774e-01,\n",
       "       -5.71506143e-01,  1.74523696e-01, -3.49796981e-01, -4.12658006e-02,\n",
       "       -5.92121780e-01,  9.35913920e-01,  1.17991567e-01,  5.62198758e-01,\n",
       "       -3.72251242e-01,  7.48511434e-01, -5.85671842e-01,  5.99284112e-01,\n",
       "       -4.19356316e-01,  8.60666782e-02,  7.13525057e-01, -8.62761080e-01,\n",
       "        2.57612884e-01,  4.63201851e-01, -3.24389517e-01, -3.87123615e-01,\n",
       "        1.26571208e-01, -4.87368673e-01, -1.44880712e-01,  3.96623045e-01,\n",
       "        9.33789760e-02, -7.66997457e-01,  1.52846411e-01,  2.18596622e-01,\n",
       "        1.03887379e+00, -4.50378895e-01, -4.82612550e-01,  5.47775984e-01,\n",
       "       -7.64466524e-02, -2.75666267e-02, -2.11762249e-01,  2.41730228e-01,\n",
       "       -2.00064719e-01, -1.00313950e+00,  5.30778348e-01, -8.17351460e-01,\n",
       "        2.32117716e-02,  1.94842577e-01,  2.77003080e-01, -1.59762874e-01,\n",
       "       -2.09863737e-01,  4.31712657e-01, -8.37902546e-01,  2.25763321e-02,\n",
       "       -1.50683016e-01, -1.69860899e-01, -4.66837510e-02, -4.68170047e-01,\n",
       "        2.99406499e-01,  2.24031527e-02,  1.15544796e-01,  4.00306076e-01,\n",
       "        9.41042840e-01,  3.78793329e-01, -2.34509110e-01, -3.30593228e-01,\n",
       "       -4.55935538e-01,  3.79622936e-01,  5.43193579e-01,  6.02495670e-02,\n",
       "        3.68198454e-01,  2.76881367e-01, -1.56596959e-01, -6.57819748e-01,\n",
       "        1.58715583e-02,  1.67994529e-01,  7.90990070e-02,  1.54479161e-01,\n",
       "        4.90975320e-01,  2.75600165e-01, -8.77218902e-01,  1.77820429e-01,\n",
       "        6.94642439e-02, -5.61373830e-01,  5.18324018e-01, -3.10645670e-01,\n",
       "        1.85658365e-01,  8.25923085e-02,  8.61015439e-01,  2.04244629e-01,\n",
       "        4.75536883e-01,  6.87890291e-01, -6.20347142e-01, -1.23703569e-01,\n",
       "        8.43648374e-01, -2.63560951e-01,  3.93254668e-01,  1.75094768e-01,\n",
       "       -1.26994801e+00,  1.00738525e+00,  3.38514857e-02, -3.47852170e-01,\n",
       "       -2.11264908e-01, -1.11962743e-02,  1.88761592e-01, -1.66788995e-02,\n",
       "        5.68394899e-01, -4.58167523e-01,  1.30343676e+00,  3.29140484e-01,\n",
       "        5.46641469e-01, -3.21961045e-02,  2.89696574e-01,  1.48957849e-01,\n",
       "        1.99962303e-01,  2.41495743e-01,  1.33437943e+00,  2.12382510e-01,\n",
       "       -1.36262089e-01, -4.51153487e-01,  7.00960338e-01, -2.55881160e-01,\n",
       "       -2.59274423e-01,  5.78921288e-02, -1.41588151e-01,  1.10462356e+00,\n",
       "        7.51244053e-02,  2.35104501e-01, -2.87890106e-01,  2.58328378e-01,\n",
       "       -1.03069150e+00,  1.06716383e+00, -4.04172599e-01,  4.79335599e-02,\n",
       "       -9.84776855e-01, -8.76864791e-02,  3.04778397e-01, -7.15065598e-01,\n",
       "        5.34994960e-01,  9.05356944e-01,  9.52498198e-01, -5.50198331e-02,\n",
       "       -1.20324224e-01, -4.17085886e-01,  1.07436311e+00,  5.77311277e-01,\n",
       "        7.36602068e-01,  4.23544616e-01, -2.73052990e-01,  3.25897187e-01,\n",
       "       -3.88508797e-01, -3.46491098e-01, -5.67286983e-02, -4.33420449e-01,\n",
       "       -5.11853322e-02, -4.95139658e-01,  2.81705379e-01,  7.30852544e-01,\n",
       "        2.33443618e-01, -2.26418138e-01,  1.03887029e-01,  9.29237902e-03,\n",
       "       -2.92512089e-01,  3.44494700e-01,  2.16404170e-01, -4.23719827e-03,\n",
       "        4.13833499e-01, -2.13304728e-01, -3.54139686e-01,  5.84296137e-03,\n",
       "        1.50866294e-02,  2.84027666e-01,  3.78559947e-01,  1.98405072e-01,\n",
       "       -7.75684237e-01,  1.83305115e-01, -4.69022721e-01,  1.06922364e+00,\n",
       "       -5.05056143e-01,  3.14204276e-01,  1.98298842e-01, -7.17022493e-02,\n",
       "        1.46759883e-01,  3.48679006e-01, -1.89368621e-01, -3.63281667e-01,\n",
       "       -3.87411803e-01, -3.29566807e-01, -5.20225763e-01,  1.04127157e+00,\n",
       "        6.45576835e-01, -1.19615388e+00, -3.86946239e-02,  8.02033186e-01,\n",
       "        3.74546587e-01, -2.84893245e-01, -2.37130046e-01,  1.45654678e-02,\n",
       "        6.64201826e-02,  4.17400718e-01,  1.79777056e-01,  1.18220717e-01,\n",
       "       -4.44052339e-01,  4.48320538e-01, -9.31756258e-01, -5.55737793e-01,\n",
       "        1.20350704e-01,  5.34013689e-01, -2.35694230e-01,  2.27902338e-01,\n",
       "       -8.65200609e-02, -3.15869600e-01, -1.51770458e-01, -6.60384536e-01,\n",
       "       -1.54837877e-01, -1.32897389e+00,  3.83679755e-02, -1.97420418e-01,\n",
       "        6.28133476e-01,  4.46448952e-01, -1.30691543e-01, -2.68271506e-01,\n",
       "        8.98965299e-02, -4.23212051e-01,  1.44195080e-01, -1.03101182e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vector(\"the flower is beautiful\", \"flower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v:k for k, v in processor.full_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowers vs. Insects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All borrowed from WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(wlist, filter_oov=True):\n",
    "    return [w.strip() for w in wlist.lower().replace(\"\\n\", \" \").split(\", \") if w.strip() in rev_vocab or not filter_oov]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words not in vocab are removed and target words are converted to adjectives when applicable and removed otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flower_words = to_words(\"\"\"aster, clover, hyacinth, marigold, poppy, azalea, crocus, iris, orchid, rose, bluebell, daffodil, lilac, pansy, tulip, buttercup, daisy, lily, peony, violet, carnation, gladiola,\n",
    "# magnolia, petunia, zinnia\"\"\")\n",
    "# insect_words = to_words(\"\"\"ant, caterpillar, flea, locust, spider, bedbug, centipede, fly, maggot, tarantula,\n",
    "# bee, cockroach, gnat, mosquito, termite, beetle, cricket, hornet, moth, wasp, blackfly,\n",
    "# dragonfly, horsefly, roach, weevil\"\"\")\n",
    "flower_single_words = [\"flower\"]\n",
    "flower_words = [\"flowers\"]\n",
    "insect_single_words = [\"bug\"]\n",
    "insect_words = [\"bugs\"]\n",
    "pleasant_words = to_words(\"\"\"caress, freedom, health, love, peace, cheer, friend, heaven, loyal, pleasure, diamond, gentle, honest, lucky, rainbow, diploma, gift, honor, miracle, sunrise, family,\n",
    "happy, laughter, paradise, vacation\"\"\", filter_oov=False)\n",
    "unpleasant_words = to_words(\"\"\"abuse, crash, filth, murder, sickness, accident, death, grief, poison, stink,\n",
    "assault, disaster, hatred, pollute, tragedy, divorce, jail, poverty, ugly, cancer, kill, rotten,\n",
    "vomit, agony, prison\"\"\", filter_oov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stimulus': 'beautiful',\n",
       " 'bias': 5.321797706793324,\n",
       " 'prior_correction': 3.5636120069576354,\n",
       " 'bias_prior_corrected': 1.758185699835689}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG are XXX.\", [flower_words, insect_words], \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stimulus': 'pleasant',\n",
       " 'bias': 4.155169996205565,\n",
       " 'prior_correction': 3.5636120069576354,\n",
       " 'bias_prior_corrected': 0.5915579892479297}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_score(\"GGG are XXX.\", [flower_words, insect_words], \"pleasant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.534074</td>\n",
       "      <td>-5.327127</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.931570</td>\n",
       "      <td>-7.929632</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935260</td>\n",
       "      <td>-8.925941</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.969309</td>\n",
       "      <td>-6.891893</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.585581</td>\n",
       "      <td>-7.275620</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.589396</td>\n",
       "      <td>-8.271805</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.620295</td>\n",
       "      <td>-7.240907</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.981358</td>\n",
       "      <td>-6.879843</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.540774</td>\n",
       "      <td>-10.401975</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.312522</td>\n",
       "      <td>-7.548679</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.466475</td>\n",
       "      <td>-6.394727</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.188142</td>\n",
       "      <td>-8.673060</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.053281</td>\n",
       "      <td>-7.807920</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.306459</td>\n",
       "      <td>-11.167660</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.155793</td>\n",
       "      <td>-2.705408</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.613336</td>\n",
       "      <td>-6.247866</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.169296</td>\n",
       "      <td>-4.691905</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.403495</td>\n",
       "      <td>-5.457706</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.007300</td>\n",
       "      <td>-6.853902</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.032389</td>\n",
       "      <td>-7.828813</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.124524</td>\n",
       "      <td>-7.736678</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.289851</td>\n",
       "      <td>-10.151052</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.341104</td>\n",
       "      <td>-9.520097</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.364365</td>\n",
       "      <td>-7.496836</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.336958</td>\n",
       "      <td>-8.524243</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.197660</td>\n",
       "      <td>-0.355556</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191879</td>\n",
       "      <td>-2.361338</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.413348</td>\n",
       "      <td>-3.139868</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.916657</td>\n",
       "      <td>0.363440</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.864190</td>\n",
       "      <td>-0.689027</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.455996</td>\n",
       "      <td>-2.097220</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.278081</td>\n",
       "      <td>-3.275135</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.241117</td>\n",
       "      <td>-0.312099</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.096925</td>\n",
       "      <td>-3.456292</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.010139</td>\n",
       "      <td>-0.543077</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.192854</td>\n",
       "      <td>1.639637</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.448967</td>\n",
       "      <td>-2.104249</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.060475</td>\n",
       "      <td>-2.492742</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.506521</td>\n",
       "      <td>-3.046696</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.737398</td>\n",
       "      <td>3.184181</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.498152</td>\n",
       "      <td>-2.055065</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.540969</td>\n",
       "      <td>0.987752</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.504524</td>\n",
       "      <td>0.951308</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.502133</td>\n",
       "      <td>-1.051084</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.639030</td>\n",
       "      <td>-1.914187</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.361930</td>\n",
       "      <td>-3.191287</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.273916</td>\n",
       "      <td>-2.279301</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.051201</td>\n",
       "      <td>-2.502015</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.119543</td>\n",
       "      <td>-0.433674</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.242821</td>\n",
       "      <td>-4.796038</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0   4.534074             -5.327127          9.861201    caress\n",
       "1   1.931570             -7.929632          9.861201   freedom\n",
       "2   0.935260             -8.925941          9.861201    health\n",
       "3   2.969309             -6.891893          9.861201      love\n",
       "4   2.585581             -7.275620          9.861201     peace\n",
       "5   1.589396             -8.271805          9.861201     cheer\n",
       "6   2.620295             -7.240907          9.861201    friend\n",
       "7   2.981358             -6.879843          9.861201    heaven\n",
       "8  -0.540774            -10.401975          9.861201     loyal\n",
       "9   2.312522             -7.548679          9.861201  pleasure\n",
       "10  3.466475             -6.394727          9.861201   diamond\n",
       "11  1.188142             -8.673060          9.861201    gentle\n",
       "12  2.053281             -7.807920          9.861201    honest\n",
       "13 -1.306459            -11.167660          9.861201     lucky\n",
       "14  7.155793             -2.705408          9.861201   rainbow\n",
       "15  3.613336             -6.247866          9.861201   diploma\n",
       "16  5.169296             -4.691905          9.861201      gift\n",
       "17  4.403495             -5.457706          9.861201     honor\n",
       "18  3.007300             -6.853902          9.861201   miracle\n",
       "19  2.032389             -7.828813          9.861201   sunrise\n",
       "20  2.124524             -7.736678          9.861201    family\n",
       "21 -0.289851            -10.151052          9.861201     happy\n",
       "22  0.341104             -9.520097          9.861201  laughter\n",
       "23  2.364365             -7.496836          9.861201  paradise\n",
       "24  1.336958             -8.524243          9.861201  vacation\n",
       "0   3.197660             -0.355556          3.553216    caress\n",
       "1   1.191879             -2.361338          3.553216   freedom\n",
       "2   0.413348             -3.139868          3.553216    health\n",
       "3   3.916657              0.363440          3.553216      love\n",
       "4   2.864190             -0.689027          3.553216     peace\n",
       "5   1.455996             -2.097220          3.553216     cheer\n",
       "6   0.278081             -3.275135          3.553216    friend\n",
       "7   3.241117             -0.312099          3.553216    heaven\n",
       "8   0.096925             -3.456292          3.553216     loyal\n",
       "9   3.010139             -0.543077          3.553216  pleasure\n",
       "10  5.192854              1.639637          3.553216   diamond\n",
       "11  1.448967             -2.104249          3.553216    gentle\n",
       "12  1.060475             -2.492742          3.553216    honest\n",
       "13  0.506521             -3.046696          3.553216     lucky\n",
       "14  6.737398              3.184181          3.553216   rainbow\n",
       "15  1.498152             -2.055065          3.553216   diploma\n",
       "16  4.540969              0.987752          3.553216      gift\n",
       "17  4.504524              0.951308          3.553216     honor\n",
       "18  2.502133             -1.051084          3.553216   miracle\n",
       "19  1.639030             -1.914187          3.553216   sunrise\n",
       "20  0.361930             -3.191287          3.553216    family\n",
       "21  1.273916             -2.279301          3.553216     happy\n",
       "22  1.051201             -2.502015          3.553216  laughter\n",
       "23  3.119543             -0.433674          3.553216  paradise\n",
       "24 -1.242821             -4.796038          3.553216  vacation"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"the GGG is XXX.\", \n",
    "                         [flower_words, insect_words], w) for w in pleasant_words]),\n",
    "pd.DataFrame([bias_score(\"GGG are XXX.\", \n",
    "                         [flower_single_words, insect_single_words], w) for w in pleasant_words]),\n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.458418484485658"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314223</td>\n",
       "      <td>-9.546979</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.594507</td>\n",
       "      <td>-9.266695</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027894</td>\n",
       "      <td>-9.833307</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.106988</td>\n",
       "      <td>-9.968189</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.446697</td>\n",
       "      <td>-10.307898</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.296808</td>\n",
       "      <td>-9.564393</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.649060</td>\n",
       "      <td>-9.212141</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.743161</td>\n",
       "      <td>-8.118040</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.930886</td>\n",
       "      <td>-8.930315</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.206610</td>\n",
       "      <td>-10.067811</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.033607</td>\n",
       "      <td>-9.827595</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.044309</td>\n",
       "      <td>-9.816892</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.324912</td>\n",
       "      <td>-9.536289</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.991967</td>\n",
       "      <td>-3.869235</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.680380</td>\n",
       "      <td>-8.180821</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.047873</td>\n",
       "      <td>-7.813329</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.699905</td>\n",
       "      <td>-10.561106</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.945634</td>\n",
       "      <td>-11.806835</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.830495</td>\n",
       "      <td>-8.030706</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.102906</td>\n",
       "      <td>-9.758295</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.330101</td>\n",
       "      <td>-9.531100</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.289234</td>\n",
       "      <td>-7.571967</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.527847</td>\n",
       "      <td>-9.333355</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.575409</td>\n",
       "      <td>-8.285792</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.538119</td>\n",
       "      <td>-10.399321</td>\n",
       "      <td>9.861201</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.447289</td>\n",
       "      <td>-3.105927</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.197244</td>\n",
       "      <td>-4.750460</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.587515</td>\n",
       "      <td>-1.965701</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.329299</td>\n",
       "      <td>-3.223917</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.587578</td>\n",
       "      <td>-4.140795</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.119428</td>\n",
       "      <td>-4.672645</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.103615</td>\n",
       "      <td>-1.449602</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.998106</td>\n",
       "      <td>-1.555111</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.807837</td>\n",
       "      <td>-1.745379</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.696869</td>\n",
       "      <td>-4.250085</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.669645</td>\n",
       "      <td>-5.222862</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.049758</td>\n",
       "      <td>-2.503459</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.717680</td>\n",
       "      <td>-1.835536</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.139421</td>\n",
       "      <td>0.586205</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.008451</td>\n",
       "      <td>-2.544765</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.278175</td>\n",
       "      <td>-3.275041</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.504257</td>\n",
       "      <td>-5.057474</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.889813</td>\n",
       "      <td>-4.443030</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.439070</td>\n",
       "      <td>-2.114147</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.082582</td>\n",
       "      <td>-3.635799</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.425654</td>\n",
       "      <td>-3.127562</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.582447</td>\n",
       "      <td>-1.970770</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.905866</td>\n",
       "      <td>-2.647351</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.881909</td>\n",
       "      <td>-1.671307</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003123</td>\n",
       "      <td>-3.550094</td>\n",
       "      <td>3.553216</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0   0.314223             -9.546979          9.861201     abuse\n",
       "1   0.594507             -9.266695          9.861201     crash\n",
       "2   0.027894             -9.833307          9.861201     filth\n",
       "3  -0.106988             -9.968189          9.861201    murder\n",
       "4  -0.446697            -10.307898          9.861201  sickness\n",
       "5   0.296808             -9.564393          9.861201  accident\n",
       "6   0.649060             -9.212141          9.861201     death\n",
       "7   1.743161             -8.118040          9.861201     grief\n",
       "8   0.930886             -8.930315          9.861201    poison\n",
       "9  -0.206610            -10.067811          9.861201     stink\n",
       "10  0.033607             -9.827595          9.861201   assault\n",
       "11  0.044309             -9.816892          9.861201  disaster\n",
       "12  0.324912             -9.536289          9.861201    hatred\n",
       "13  5.991967             -3.869235          9.861201   pollute\n",
       "14  1.680380             -8.180821          9.861201   tragedy\n",
       "15  2.047873             -7.813329          9.861201   divorce\n",
       "16 -0.699905            -10.561106          9.861201      jail\n",
       "17 -1.945634            -11.806835          9.861201   poverty\n",
       "18  1.830495             -8.030706          9.861201      ugly\n",
       "19  0.102906             -9.758295          9.861201    cancer\n",
       "20  0.330101             -9.531100          9.861201      kill\n",
       "21  2.289234             -7.571967          9.861201    rotten\n",
       "22  0.527847             -9.333355          9.861201     vomit\n",
       "23  1.575409             -8.285792          9.861201     agony\n",
       "24 -0.538119            -10.399321          9.861201    prison\n",
       "0   0.447289             -3.105927          3.553216     abuse\n",
       "1  -1.197244             -4.750460          3.553216     crash\n",
       "2   1.587515             -1.965701          3.553216     filth\n",
       "3   0.329299             -3.223917          3.553216    murder\n",
       "4  -0.587578             -4.140795          3.553216  sickness\n",
       "5  -1.119428             -4.672645          3.553216  accident\n",
       "6   2.103615             -1.449602          3.553216     death\n",
       "7   1.998106             -1.555111          3.553216     grief\n",
       "8   1.807837             -1.745379          3.553216    poison\n",
       "9  -0.696869             -4.250085          3.553216     stink\n",
       "10 -1.669645             -5.222862          3.553216   assault\n",
       "11  1.049758             -2.503459          3.553216  disaster\n",
       "12  1.717680             -1.835536          3.553216    hatred\n",
       "13  4.139421              0.586205          3.553216   pollute\n",
       "14  1.008451             -2.544765          3.553216   tragedy\n",
       "15  0.278175             -3.275041          3.553216   divorce\n",
       "16 -1.504257             -5.057474          3.553216      jail\n",
       "17 -0.889813             -4.443030          3.553216   poverty\n",
       "18  1.439070             -2.114147          3.553216      ugly\n",
       "19 -0.082582             -3.635799          3.553216    cancer\n",
       "20  0.425654             -3.127562          3.553216      kill\n",
       "21  1.582447             -1.970770          3.553216    rotten\n",
       "22  0.905866             -2.647351          3.553216     vomit\n",
       "23  1.881909             -1.671307          3.553216     agony\n",
       "24  0.003123             -3.550094          3.553216    prison"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"the GGG is XXX.\", \n",
    "                         [flower_words, insect_words], w) for w in unpleasant_words]),\n",
    "pd.DataFrame([bias_score(\"GGG are XXX.\", \n",
    "                         [flower_single_words, insect_single_words], w) for w in unpleasant_words]),\n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.060220403734268"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical test (is the t-test appropriate here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44577436955883154"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=2.275411770775029, pvalue=0.025059068460106658)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=2.3714740371572467, pvalue=0.017717291395921632)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02494"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] are {x}\", x) for x in pleasant_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] are {x}\", x) for x in unpleasant_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13553564"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_insect = get_word_vector(\"insects are [MASK]\", \"insects\")\n",
    "sims_insect1 = [cosine_similarity(wv_insect, wv) for wv in wvs1]\n",
    "sims_insect2 = [cosine_similarity(wv_insect, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_insect1) - np.mean(sims_insect2)\n",
    "std_ = np.std(sims_insect1 + sims_insect2)\n",
    "effect_sz_insect = mean_diff / std_; effect_sz_insect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19293566"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_flower = get_word_vector(\"flowers are [MASK]\", \"flowers\")\n",
    "sims_flower1 = [cosine_similarity(wv_flower, wv) for wv in wvs1]\n",
    "sims_flower2 = [cosine_similarity(wv_flower, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_flower1) - np.mean(sims_flower2)\n",
    "std_ = np.std(sims_flower1 + sims_flower2)\n",
    "effect_sz_flower = mean_diff / std_; effect_sz_flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_insect1, sims_flower1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_insect2, sims_flower2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career vs Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = to_words(\"he\")\n",
    "female_words = to_words(\"she\")\n",
    "# male_words = to_words(\"John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill\".lower())\n",
    "# female_words = to_words(\"Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna\".lower())\n",
    "male_plural_words = to_words(\"boys, men\")\n",
    "female_plural_words = to_words(\"girls, women\")\n",
    "career_words = to_words(\"executive, management, professional, corporation, salary, office, business, career\")\n",
    "family_words = to_words(\"home, parents, children, family, cousins, marriage, wedding, relatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.599076</td>\n",
       "      <td>-0.067961</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705400</td>\n",
       "      <td>0.038362</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.655194</td>\n",
       "      <td>-0.011843</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.832335</td>\n",
       "      <td>1.165297</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.705611</td>\n",
       "      <td>1.038574</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.620151</td>\n",
       "      <td>-0.046886</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.630229</td>\n",
       "      <td>-0.036809</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.301032</td>\n",
       "      <td>0.633995</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.205276</td>\n",
       "      <td>-0.448839</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.220119</td>\n",
       "      <td>-0.433996</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.496712</td>\n",
       "      <td>-1.150827</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.265399</td>\n",
       "      <td>0.611284</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.609615</td>\n",
       "      <td>-0.044500</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.070765</td>\n",
       "      <td>-0.583350</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.002690</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.296034</td>\n",
       "      <td>-0.358081</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859383</td>\n",
       "      <td>-0.720847</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.788202</td>\n",
       "      <td>-0.792028</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503228</td>\n",
       "      <td>-1.077002</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.359952</td>\n",
       "      <td>-0.220278</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.654332</td>\n",
       "      <td>-0.925898</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.623051</td>\n",
       "      <td>-0.957179</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.746953</td>\n",
       "      <td>-0.833277</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.908056</td>\n",
       "      <td>-0.672174</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction      stimulus\n",
       "0  0.599076             -0.067961          0.667037     executive\n",
       "1  0.705400              0.038362          0.667037    management\n",
       "2  0.655194             -0.011843          0.667037  professional\n",
       "3  1.832335              1.165297          0.667037   corporation\n",
       "4  1.705611              1.038574          0.667037        salary\n",
       "5  0.620151             -0.046886          0.667037        office\n",
       "6  0.630229             -0.036809          0.667037      business\n",
       "7  1.301032              0.633995          0.667037        career\n",
       "0  0.205276             -0.448839          0.654115     executive\n",
       "1  0.220119             -0.433996          0.654115    management\n",
       "2 -0.496712             -1.150827          0.654115  professional\n",
       "3  1.265399              0.611284          0.654115   corporation\n",
       "4  0.609615             -0.044500          0.654115        salary\n",
       "5  0.070765             -0.583350          0.654115        office\n",
       "6 -0.002690             -0.656805          0.654115      business\n",
       "7  0.296034             -0.358081          0.654115        career\n",
       "0  0.859383             -0.720847          1.580230     executive\n",
       "1  0.788202             -0.792028          1.580230    management\n",
       "2  0.503228             -1.077002          1.580230  professional\n",
       "3  1.359952             -0.220278          1.580230   corporation\n",
       "4  0.654332             -0.925898          1.580230        salary\n",
       "5  0.623051             -0.957179          1.580230        office\n",
       "6  0.746953             -0.833277          1.580230      business\n",
       "7  0.908056             -0.672174          1.580230        career"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in career_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in career_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in career_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2729611971015083"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.262431</td>\n",
       "      <td>-0.929469</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.137538</td>\n",
       "      <td>-0.804575</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000792</td>\n",
       "      <td>-0.667829</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.536999</td>\n",
       "      <td>-0.130039</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335162</td>\n",
       "      <td>-0.331875</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>cousins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.090113</td>\n",
       "      <td>-0.576925</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.150405</td>\n",
       "      <td>-0.516633</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.260326</td>\n",
       "      <td>-0.406711</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>relatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.204501</td>\n",
       "      <td>-0.858616</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.628228</td>\n",
       "      <td>-1.282343</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.209043</td>\n",
       "      <td>-0.863158</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.523118</td>\n",
       "      <td>-1.177233</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.269899</td>\n",
       "      <td>-0.924014</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>cousins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.118059</td>\n",
       "      <td>-0.772174</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.448021</td>\n",
       "      <td>-2.102137</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.910688</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>relatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271712</td>\n",
       "      <td>-1.308518</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084235</td>\n",
       "      <td>-1.495995</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>parents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.201236</td>\n",
       "      <td>-1.781466</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.408172</td>\n",
       "      <td>-1.172058</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335865</td>\n",
       "      <td>-1.244365</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>cousins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.130142</td>\n",
       "      <td>-1.710372</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.095081</td>\n",
       "      <td>-1.675311</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>wedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.063976</td>\n",
       "      <td>-1.516254</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>relatives</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction   stimulus\n",
       "0 -0.262431             -0.929469          0.667037       home\n",
       "1 -0.137538             -0.804575          0.667037    parents\n",
       "2 -0.000792             -0.667829          0.667037   children\n",
       "3  0.536999             -0.130039          0.667037     family\n",
       "4  0.335162             -0.331875          0.667037    cousins\n",
       "5  0.090113             -0.576925          0.667037   marriage\n",
       "6  0.150405             -0.516633          0.667037    wedding\n",
       "7  0.260326             -0.406711          0.667037  relatives\n",
       "0 -0.204501             -0.858616          0.654115       home\n",
       "1 -0.628228             -1.282343          0.654115    parents\n",
       "2 -0.209043             -0.863158          0.654115   children\n",
       "3 -0.523118             -1.177233          0.654115     family\n",
       "4 -0.269899             -0.924014          0.654115    cousins\n",
       "5 -0.118059             -0.772174          0.654115   marriage\n",
       "6 -1.448021             -2.102137          0.654115    wedding\n",
       "7 -0.256573             -0.910688          0.654115  relatives\n",
       "0  0.271712             -1.308518          1.580230       home\n",
       "1  0.084235             -1.495995          1.580230    parents\n",
       "2 -0.201236             -1.781466          1.580230   children\n",
       "3  0.408172             -1.172058          1.580230     family\n",
       "4  0.335865             -1.244365          1.580230    cousins\n",
       "5 -0.130142             -1.710372          1.580230   marriage\n",
       "6 -0.095081             -1.675311          1.580230    wedding\n",
       "7  0.063976             -1.516254          1.580230  relatives"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in family_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in family_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in family_words]), \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0482815581360148"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias_prior_corrected\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1255236156571247"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=4.689448872683833, pvalue=2.472253402276604e-05)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=3.93835362197209, pvalue=8.204262073313446e-05)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in family_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in family_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in family_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in career_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in career_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in career_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.587034"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_fm = get_word_vector(\"women like [MASK]\", \"women\")\n",
    "wv_fm2 = get_word_vector(\"she likes [MASK]\", \"she\")\n",
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_family_career = mean_diff / std_; effect_sz_fm_family_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24955648"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_m = get_word_vector(\"men like [MASK]\", \"men\")\n",
    "wv_m2 = get_word_vector(\"he likes [MASK]\", \"he\")\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1]+\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_m1) - np.mean(sims_m2)\n",
    "std_ = np.std(sims_m1 + sims_m1)\n",
    "effect_sz_m_family_career = mean_diff / std_; effect_sz_m_family_career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00054"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92882"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math vs. Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_words = to_words(\"math, algebra, geometry, calculus, equations, computation, numbers, addition\")\n",
    "art_words = to_words(\"poetry, art, dance, Shakespear, literature, novels, symphony, drama, sculptures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272837</td>\n",
       "      <td>-0.394201</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.495616</td>\n",
       "      <td>-0.171422</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376720</td>\n",
       "      <td>-0.290317</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.359229</td>\n",
       "      <td>-0.307808</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.007015</td>\n",
       "      <td>0.339978</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>equations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.119367</td>\n",
       "      <td>0.452329</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.606421</td>\n",
       "      <td>-0.060616</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.059602</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.253877</td>\n",
       "      <td>-0.907992</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.523835</td>\n",
       "      <td>-1.177950</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.390305</td>\n",
       "      <td>-1.044420</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.975975</td>\n",
       "      <td>-1.630090</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257568</td>\n",
       "      <td>-0.396547</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>equations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.318912</td>\n",
       "      <td>-0.335203</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.302348</td>\n",
       "      <td>-0.351767</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.230889</td>\n",
       "      <td>-0.885004</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.498302</td>\n",
       "      <td>-1.081929</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.411591</td>\n",
       "      <td>-0.168640</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.123698</td>\n",
       "      <td>-0.456532</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.074119</td>\n",
       "      <td>-0.506112</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>calculus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.690857</td>\n",
       "      <td>0.110627</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>equations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.465523</td>\n",
       "      <td>-0.114707</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>computation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.123959</td>\n",
       "      <td>-0.456272</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.787368</td>\n",
       "      <td>-0.792862</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction     stimulus\n",
       "0  0.272837             -0.394201          0.667037         math\n",
       "1  0.495616             -0.171422          0.667037      algebra\n",
       "2  0.376720             -0.290317          0.667037     geometry\n",
       "3  0.359229             -0.307808          0.667037     calculus\n",
       "4  1.007015              0.339978          0.667037    equations\n",
       "5  1.119367              0.452329          0.667037  computation\n",
       "6  0.606421             -0.060616          0.667037      numbers\n",
       "7  0.726639              0.059602          0.667037     addition\n",
       "0 -0.253877             -0.907992          0.654115         math\n",
       "1 -0.523835             -1.177950          0.654115      algebra\n",
       "2 -0.390305             -1.044420          0.654115     geometry\n",
       "3 -0.975975             -1.630090          0.654115     calculus\n",
       "4  0.257568             -0.396547          0.654115    equations\n",
       "5  0.318912             -0.335203          0.654115  computation\n",
       "6  0.302348             -0.351767          0.654115      numbers\n",
       "7 -0.230889             -0.885004          0.654115     addition\n",
       "0  0.498302             -1.081929          1.580230         math\n",
       "1  1.411591             -0.168640          1.580230      algebra\n",
       "2  1.123698             -0.456532          1.580230     geometry\n",
       "3  1.074119             -0.506112          1.580230     calculus\n",
       "4  1.690857              0.110627          1.580230    equations\n",
       "5  1.465523             -0.114707          1.580230  computation\n",
       "6  1.123959             -0.456272          1.580230      numbers\n",
       "7  0.787368             -0.792862          1.580230     addition"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in math_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in math_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in math_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5268002618517322"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.354761</td>\n",
       "      <td>-0.312276</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.062777</td>\n",
       "      <td>-0.729814</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088697</td>\n",
       "      <td>-0.578340</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.626061</td>\n",
       "      <td>-0.040976</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306323</td>\n",
       "      <td>-0.360714</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970464</td>\n",
       "      <td>0.303426</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195014</td>\n",
       "      <td>-0.472023</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375476</td>\n",
       "      <td>-0.291562</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322519</td>\n",
       "      <td>-0.976634</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.480285</td>\n",
       "      <td>-1.134400</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.050046</td>\n",
       "      <td>-1.704161</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.247654</td>\n",
       "      <td>-0.901769</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.250756</td>\n",
       "      <td>-0.904871</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.487095</td>\n",
       "      <td>-1.141210</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.304311</td>\n",
       "      <td>-1.958426</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.013455</td>\n",
       "      <td>-0.667570</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098519</td>\n",
       "      <td>-1.481711</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.172857</td>\n",
       "      <td>-1.407373</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046588</td>\n",
       "      <td>-1.626818</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363980</td>\n",
       "      <td>-1.216250</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089294</td>\n",
       "      <td>-1.490936</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.059014</td>\n",
       "      <td>-0.521216</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.165875</td>\n",
       "      <td>-1.746105</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132910</td>\n",
       "      <td>-1.447320</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction    stimulus\n",
       "0  0.354761             -0.312276          0.667037      poetry\n",
       "1 -0.062777             -0.729814          0.667037         art\n",
       "2  0.088697             -0.578340          0.667037       dance\n",
       "3  0.626061             -0.040976          0.667037  literature\n",
       "4  0.306323             -0.360714          0.667037      novels\n",
       "5  0.970464              0.303426          0.667037    symphony\n",
       "6  0.195014             -0.472023          0.667037       drama\n",
       "7  0.375476             -0.291562          0.667037  sculptures\n",
       "0 -0.322519             -0.976634          0.654115      poetry\n",
       "1 -0.480285             -1.134400          0.654115         art\n",
       "2 -1.050046             -1.704161          0.654115       dance\n",
       "3 -0.247654             -0.901769          0.654115  literature\n",
       "4 -0.250756             -0.904871          0.654115      novels\n",
       "5 -0.487095             -1.141210          0.654115    symphony\n",
       "6 -1.304311             -1.958426          0.654115       drama\n",
       "7 -0.013455             -0.667570          0.654115  sculptures\n",
       "0  0.098519             -1.481711          1.580230      poetry\n",
       "1  0.172857             -1.407373          1.580230         art\n",
       "2 -0.046588             -1.626818          1.580230       dance\n",
       "3  0.363980             -1.216250          1.580230  literature\n",
       "4  0.089294             -1.490936          1.580230      novels\n",
       "5  1.059014             -0.521216          1.580230    symphony\n",
       "6 -0.165875             -1.746105          1.580230       drama\n",
       "7  0.132910             -1.447320          1.580230  sculptures"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in art_words]),  \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01675048846096418"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8495404566140962"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=3.2235255366209885, pvalue=0.0023302895495397186)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=2.989849608303419, pvalue=0.002791148334271272)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00236"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in art_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in math_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in math_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in math_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07610056"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_art_math = mean_diff / std_; effect_sz_fm_art_math\n",
    "\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_m_art_math = mean_diff / std_; effect_sz_m_art_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70627"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96164"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science vs. Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_words = to_words(\"science, technology, physics, chemistry, Einstein, NASA, experiments, astronomy\")\n",
    "art_words = to_words(\"poetry, art, dance, Shakespear, literature, novels, symphony, drama, sculptures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631545</td>\n",
       "      <td>-0.035492</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876075</td>\n",
       "      <td>0.209037</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546070</td>\n",
       "      <td>-0.120967</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127346</td>\n",
       "      <td>-0.539691</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277876</td>\n",
       "      <td>-0.389162</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.020043</td>\n",
       "      <td>0.353006</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.982217</td>\n",
       "      <td>0.315179</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.386326</td>\n",
       "      <td>-0.280712</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.677617</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.271394</td>\n",
       "      <td>-0.382722</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.186840</td>\n",
       "      <td>-0.840955</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.654192</td>\n",
       "      <td>-1.308307</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.031780</td>\n",
       "      <td>0.377665</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540941</td>\n",
       "      <td>-0.113174</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.173071</td>\n",
       "      <td>-0.481044</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.642302</td>\n",
       "      <td>-1.296417</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998532</td>\n",
       "      <td>-0.581698</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996837</td>\n",
       "      <td>-0.583393</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.260710</td>\n",
       "      <td>-0.319520</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.120330</td>\n",
       "      <td>-0.459900</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.716207</td>\n",
       "      <td>-0.864023</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.124195</td>\n",
       "      <td>-0.456035</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>nasa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.086175</td>\n",
       "      <td>-0.494055</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>experiments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.415932</td>\n",
       "      <td>-0.164298</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>astronomy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction     stimulus\n",
       "0  0.631545             -0.035492          0.667037      science\n",
       "1  0.876075              0.209037          0.667037   technology\n",
       "2  0.546070             -0.120967          0.667037      physics\n",
       "3  0.127346             -0.539691          0.667037    chemistry\n",
       "4  0.277876             -0.389162          0.667037     einstein\n",
       "5  1.020043              0.353006          0.667037         nasa\n",
       "6  0.982217              0.315179          0.667037  experiments\n",
       "7  0.386326             -0.280712          0.667037    astronomy\n",
       "0 -0.023502             -0.677617          0.654115      science\n",
       "1  0.271394             -0.382722          0.654115   technology\n",
       "2 -0.186840             -0.840955          0.654115      physics\n",
       "3 -0.654192             -1.308307          0.654115    chemistry\n",
       "4  1.031780              0.377665          0.654115     einstein\n",
       "5  0.540941             -0.113174          0.654115         nasa\n",
       "6  0.173071             -0.481044          0.654115  experiments\n",
       "7 -0.642302             -1.296417          0.654115    astronomy\n",
       "0  0.998532             -0.581698          1.580230      science\n",
       "1  0.996837             -0.583393          1.580230   technology\n",
       "2  1.260710             -0.319520          1.580230      physics\n",
       "3  1.120330             -0.459900          1.580230    chemistry\n",
       "4  0.716207             -0.864023          1.580230     einstein\n",
       "5  1.124195             -0.456035          1.580230         nasa\n",
       "6  1.086175             -0.494055          1.580230  experiments\n",
       "7  1.415932             -0.164298          1.580230    astronomy"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in science_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in science_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in science_words]), \n",
    "])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5865318700928992"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.354761</td>\n",
       "      <td>-0.312276</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.062777</td>\n",
       "      <td>-0.729814</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088697</td>\n",
       "      <td>-0.578340</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.626061</td>\n",
       "      <td>-0.040976</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306323</td>\n",
       "      <td>-0.360714</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970464</td>\n",
       "      <td>0.303426</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195014</td>\n",
       "      <td>-0.472023</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375476</td>\n",
       "      <td>-0.291562</td>\n",
       "      <td>0.667037</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322519</td>\n",
       "      <td>-0.976634</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.480285</td>\n",
       "      <td>-1.134400</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.050046</td>\n",
       "      <td>-1.704161</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.247654</td>\n",
       "      <td>-0.901769</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.250756</td>\n",
       "      <td>-0.904871</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.487095</td>\n",
       "      <td>-1.141210</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.304311</td>\n",
       "      <td>-1.958426</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.013455</td>\n",
       "      <td>-0.667570</td>\n",
       "      <td>0.654115</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098519</td>\n",
       "      <td>-1.481711</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.172857</td>\n",
       "      <td>-1.407373</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046588</td>\n",
       "      <td>-1.626818</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363980</td>\n",
       "      <td>-1.216250</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>literature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089294</td>\n",
       "      <td>-1.490936</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.059014</td>\n",
       "      <td>-0.521216</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>symphony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.165875</td>\n",
       "      <td>-1.746105</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.132910</td>\n",
       "      <td>-1.447320</td>\n",
       "      <td>1.580230</td>\n",
       "      <td>sculptures</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias  bias_prior_corrected  prior_correction    stimulus\n",
       "0  0.354761             -0.312276          0.667037      poetry\n",
       "1 -0.062777             -0.729814          0.667037         art\n",
       "2  0.088697             -0.578340          0.667037       dance\n",
       "3  0.626061             -0.040976          0.667037  literature\n",
       "4  0.306323             -0.360714          0.667037      novels\n",
       "5  0.970464              0.303426          0.667037    symphony\n",
       "6  0.195014             -0.472023          0.667037       drama\n",
       "7  0.375476             -0.291562          0.667037  sculptures\n",
       "0 -0.322519             -0.976634          0.654115      poetry\n",
       "1 -0.480285             -1.134400          0.654115         art\n",
       "2 -1.050046             -1.704161          0.654115       dance\n",
       "3 -0.247654             -0.901769          0.654115  literature\n",
       "4 -0.250756             -0.904871          0.654115      novels\n",
       "5 -0.487095             -1.141210          0.654115    symphony\n",
       "6 -1.304311             -1.958426          0.654115       drama\n",
       "7 -0.013455             -0.667570          0.654115  sculptures\n",
       "0  0.098519             -1.481711          1.580230      poetry\n",
       "1  0.172857             -1.407373          1.580230         art\n",
       "2 -0.046588             -1.626818          1.580230       dance\n",
       "3  0.363980             -1.216250          1.580230  literature\n",
       "4  0.089294             -1.490936          1.580230      novels\n",
       "5  1.059014             -0.521216          1.580230    symphony\n",
       "6 -0.165875             -1.746105          1.580230       drama\n",
       "7  0.132910             -1.447320          1.580230  sculptures"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "    pd.DataFrame([bias_score(\"GGG likes XXX.\", [male_words, female_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG like XXX.\", [male_plural_words, female_plural_words], w) for w in art_words]),\n",
    "    pd.DataFrame([bias_score(\"GGG is interested in XXX.\", [[\"he\"], ['she']], w) for w in art_words]), \n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01675048846096418"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"bias\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571936118517962"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=3.7478804815913764, pvalue=0.0004965672288220781)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RanksumsResult(statistic=3.2579050904271742, pvalue=0.0011223793767071923)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranksums(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00057"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs1 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in art_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in art_words\n",
    "]\n",
    "wvs2 = [\n",
    "    get_word_vector(f\"[MASK] like {x}\", x) for x in science_words\n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] likes {x}\", x) for x in science_words    \n",
    "] + [\n",
    "    get_word_vector(f\"[MASK] is interested in {x}\", x) for x in science_words\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19403774"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_fm1 = [cosine_similarity(wv_fm, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs1]\n",
    "sims_fm2 = [cosine_similarity(wv_fm, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_fm2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_fm_art_math = mean_diff / std_; effect_sz_fm_art_math\n",
    "\n",
    "sims_m1 = [cosine_similarity(wv_m, wv) for wv in wvs1] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs1]\n",
    "sims_m2 = [cosine_similarity(wv_m, wv) for wv in wvs2] +\\\n",
    "           [cosine_similarity(wv_m2, wv) for wv in wvs2]\n",
    "mean_diff = np.mean(sims_fm1) - np.mean(sims_fm2)\n",
    "std_ = np.std(sims_fm1 + sims_fm1)\n",
    "effect_sz_m_art_math = mean_diff / std_; effect_sz_m_art_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70943"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm1, sims_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79797"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(sims_fm2, sims_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black vs. White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.343063</td>\n",
       "      <td>-0.023316</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.095751</td>\n",
       "      <td>0.223996</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.511618</td>\n",
       "      <td>-0.191871</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.491093</td>\n",
       "      <td>-0.171346</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.065176</td>\n",
       "      <td>0.254571</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.325136</td>\n",
       "      <td>-0.005389</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.646209</td>\n",
       "      <td>-0.326462</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.139130</td>\n",
       "      <td>0.180617</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.021979</td>\n",
       "      <td>0.297768</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.737702</td>\n",
       "      <td>-0.417955</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.655645</td>\n",
       "      <td>0.975393</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.374250</td>\n",
       "      <td>-0.054503</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.243326</td>\n",
       "      <td>0.076422</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.120310</td>\n",
       "      <td>0.199437</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.182806</td>\n",
       "      <td>0.136942</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.284277</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.130030</td>\n",
       "      <td>0.189717</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.549829</td>\n",
       "      <td>0.869576</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.122960</td>\n",
       "      <td>0.196787</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.369181</td>\n",
       "      <td>0.688928</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.187369</td>\n",
       "      <td>0.132379</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.252288</td>\n",
       "      <td>0.067459</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.163684</td>\n",
       "      <td>0.483431</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.242304</td>\n",
       "      <td>0.562051</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.565565</td>\n",
       "      <td>-0.245818</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.039367</td>\n",
       "      <td>0.252456</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.680022</td>\n",
       "      <td>0.611801</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>freedom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.716411</td>\n",
       "      <td>0.575411</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.540408</td>\n",
       "      <td>0.751414</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.712942</td>\n",
       "      <td>0.578880</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.205009</td>\n",
       "      <td>1.086814</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>cheer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.654000</td>\n",
       "      <td>0.637823</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.244068</td>\n",
       "      <td>1.047754</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.027376</td>\n",
       "      <td>1.264447</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>loyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.907735</td>\n",
       "      <td>0.384087</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.153675</td>\n",
       "      <td>1.445497</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>diamond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.474459</td>\n",
       "      <td>0.817363</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>gentle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.261571</td>\n",
       "      <td>1.553394</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>honest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.554630</td>\n",
       "      <td>1.846452</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>lucky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.328112</td>\n",
       "      <td>0.963710</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>rainbow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.874869</td>\n",
       "      <td>0.416953</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>diploma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.815817</td>\n",
       "      <td>0.476006</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.380243</td>\n",
       "      <td>0.911579</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>honor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.302484</td>\n",
       "      <td>0.989339</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>miracle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.854970</td>\n",
       "      <td>0.436853</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>sunrise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.944123</td>\n",
       "      <td>0.347699</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.083789</td>\n",
       "      <td>1.208033</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.336443</td>\n",
       "      <td>0.955379</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.132994</td>\n",
       "      <td>1.158828</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>paradise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.836768</td>\n",
       "      <td>0.455054</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>vacation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0  -0.343063             -0.023316         -0.319747    caress\n",
       "1  -0.095751              0.223996         -0.319747   freedom\n",
       "2  -0.511618             -0.191871         -0.319747    health\n",
       "3  -0.491093             -0.171346         -0.319747      love\n",
       "4  -0.065176              0.254571         -0.319747     peace\n",
       "5  -0.325136             -0.005389         -0.319747     cheer\n",
       "6  -0.646209             -0.326462         -0.319747    friend\n",
       "7  -0.139130              0.180617         -0.319747    heaven\n",
       "8  -0.021979              0.297768         -0.319747     loyal\n",
       "9  -0.737702             -0.417955         -0.319747  pleasure\n",
       "10  0.655645              0.975393         -0.319747   diamond\n",
       "11 -0.374250             -0.054503         -0.319747    gentle\n",
       "12 -0.243326              0.076422         -0.319747    honest\n",
       "13 -0.120310              0.199437         -0.319747     lucky\n",
       "14 -0.182806              0.136942         -0.319747   rainbow\n",
       "15 -0.284277              0.035470         -0.319747   diploma\n",
       "16 -0.130030              0.189717         -0.319747      gift\n",
       "17  0.549829              0.869576         -0.319747     honor\n",
       "18 -0.122960              0.196787         -0.319747   miracle\n",
       "19  0.369181              0.688928         -0.319747   sunrise\n",
       "20 -0.187369              0.132379         -0.319747    family\n",
       "21 -0.252288              0.067459         -0.319747     happy\n",
       "22  0.163684              0.483431         -0.319747  laughter\n",
       "23  0.242304              0.562051         -0.319747  paradise\n",
       "24 -0.565565             -0.245818         -0.319747  vacation\n",
       "0  -1.039367              0.252456         -1.291822    caress\n",
       "1  -0.680022              0.611801         -1.291822   freedom\n",
       "2  -0.716411              0.575411         -1.291822    health\n",
       "3  -0.540408              0.751414         -1.291822      love\n",
       "4  -0.712942              0.578880         -1.291822     peace\n",
       "5  -0.205009              1.086814         -1.291822     cheer\n",
       "6  -0.654000              0.637823         -1.291822    friend\n",
       "7  -0.244068              1.047754         -1.291822    heaven\n",
       "8  -0.027376              1.264447         -1.291822     loyal\n",
       "9  -0.907735              0.384087         -1.291822  pleasure\n",
       "10  0.153675              1.445497         -1.291822   diamond\n",
       "11 -0.474459              0.817363         -1.291822    gentle\n",
       "12  0.261571              1.553394         -1.291822    honest\n",
       "13  0.554630              1.846452         -1.291822     lucky\n",
       "14 -0.328112              0.963710         -1.291822   rainbow\n",
       "15 -0.874869              0.416953         -1.291822   diploma\n",
       "16 -0.815817              0.476006         -1.291822      gift\n",
       "17 -0.380243              0.911579         -1.291822     honor\n",
       "18 -0.302484              0.989339         -1.291822   miracle\n",
       "19 -0.854970              0.436853         -1.291822   sunrise\n",
       "20 -0.944123              0.347699         -1.291822    family\n",
       "21 -0.083789              1.208033         -1.291822     happy\n",
       "22 -0.336443              0.955379         -1.291822  laughter\n",
       "23 -0.132994              1.158828         -1.291822  paradise\n",
       "24 -0.836768              0.455054         -1.291822  vacation"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"GGG people are XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in pleasant_words]),\n",
    "pd.DataFrame([bias_score(\"the GGG person is XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in pleasant_words]),])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_prior_corrected</th>\n",
       "      <th>prior_correction</th>\n",
       "      <th>stimulus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.802454</td>\n",
       "      <td>1.122201</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190123</td>\n",
       "      <td>0.509870</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.322205</td>\n",
       "      <td>0.641952</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511878</td>\n",
       "      <td>0.831625</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.127268</td>\n",
       "      <td>0.192479</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.276686</td>\n",
       "      <td>0.043061</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.474574</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.547298</td>\n",
       "      <td>0.867045</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.033762</td>\n",
       "      <td>0.285985</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.952864</td>\n",
       "      <td>1.272611</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.587359</td>\n",
       "      <td>0.907106</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.518136</td>\n",
       "      <td>0.837884</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.356713</td>\n",
       "      <td>0.676460</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.122981</td>\n",
       "      <td>0.442728</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.949752</td>\n",
       "      <td>1.269499</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.412901</td>\n",
       "      <td>-0.093153</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.524382</td>\n",
       "      <td>1.844129</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.837338</td>\n",
       "      <td>1.157085</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.432732</td>\n",
       "      <td>0.752479</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.422632</td>\n",
       "      <td>0.742379</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.635643</td>\n",
       "      <td>0.955390</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.160904</td>\n",
       "      <td>0.480651</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.110129</td>\n",
       "      <td>0.429876</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.794574</td>\n",
       "      <td>1.114321</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.762466</td>\n",
       "      <td>1.082214</td>\n",
       "      <td>-0.319747</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.092688</td>\n",
       "      <td>1.199135</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051922</td>\n",
       "      <td>1.343745</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.256091</td>\n",
       "      <td>1.035731</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>filth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125386</td>\n",
       "      <td>1.417208</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.198288</td>\n",
       "      <td>1.093535</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.396344</td>\n",
       "      <td>0.895479</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.393475</td>\n",
       "      <td>1.685297</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.345863</td>\n",
       "      <td>1.637686</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>grief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.225816</td>\n",
       "      <td>1.066006</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>poison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.598145</td>\n",
       "      <td>1.889967</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>stink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.338003</td>\n",
       "      <td>0.953820</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>assault</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.015208</td>\n",
       "      <td>1.276614</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>disaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.082615</td>\n",
       "      <td>1.374437</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>hatred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.378396</td>\n",
       "      <td>0.913426</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>pollute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.629955</td>\n",
       "      <td>1.921778</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>tragedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.206206</td>\n",
       "      <td>0.085616</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>divorce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.361823</td>\n",
       "      <td>2.653645</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.690784</td>\n",
       "      <td>1.982606</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>poverty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.112307</td>\n",
       "      <td>1.179516</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.785982</td>\n",
       "      <td>0.505841</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.139679</td>\n",
       "      <td>1.431501</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.302378</td>\n",
       "      <td>0.989444</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.780099</td>\n",
       "      <td>0.511723</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>vomit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.388967</td>\n",
       "      <td>2.680789</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>agony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.491985</td>\n",
       "      <td>1.783807</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bias  bias_prior_corrected  prior_correction  stimulus\n",
       "0   0.802454              1.122201         -0.319747     abuse\n",
       "1   0.190123              0.509870         -0.319747     crash\n",
       "2   0.322205              0.641952         -0.319747     filth\n",
       "3   0.511878              0.831625         -0.319747    murder\n",
       "4  -0.127268              0.192479         -0.319747  sickness\n",
       "5  -0.276686              0.043061         -0.319747  accident\n",
       "6   0.474574              0.794321         -0.319747     death\n",
       "7   0.547298              0.867045         -0.319747     grief\n",
       "8  -0.033762              0.285985         -0.319747    poison\n",
       "9   0.952864              1.272611         -0.319747     stink\n",
       "10  0.587359              0.907106         -0.319747   assault\n",
       "11  0.518136              0.837884         -0.319747  disaster\n",
       "12  0.356713              0.676460         -0.319747    hatred\n",
       "13  0.122981              0.442728         -0.319747   pollute\n",
       "14  0.949752              1.269499         -0.319747   tragedy\n",
       "15 -0.412901             -0.093153         -0.319747   divorce\n",
       "16  1.524382              1.844129         -0.319747      jail\n",
       "17  0.837338              1.157085         -0.319747   poverty\n",
       "18  0.432732              0.752479         -0.319747      ugly\n",
       "19  0.422632              0.742379         -0.319747    cancer\n",
       "20  0.635643              0.955390         -0.319747      kill\n",
       "21  0.160904              0.480651         -0.319747    rotten\n",
       "22  0.110129              0.429876         -0.319747     vomit\n",
       "23  0.794574              1.114321         -0.319747     agony\n",
       "24  0.762466              1.082214         -0.319747    prison\n",
       "0  -0.092688              1.199135         -1.291822     abuse\n",
       "1   0.051922              1.343745         -1.291822     crash\n",
       "2  -0.256091              1.035731         -1.291822     filth\n",
       "3   0.125386              1.417208         -1.291822    murder\n",
       "4  -0.198288              1.093535         -1.291822  sickness\n",
       "5  -0.396344              0.895479         -1.291822  accident\n",
       "6   0.393475              1.685297         -1.291822     death\n",
       "7   0.345863              1.637686         -1.291822     grief\n",
       "8  -0.225816              1.066006         -1.291822    poison\n",
       "9   0.598145              1.889967         -1.291822     stink\n",
       "10 -0.338003              0.953820         -1.291822   assault\n",
       "11 -0.015208              1.276614         -1.291822  disaster\n",
       "12  0.082615              1.374437         -1.291822    hatred\n",
       "13 -0.378396              0.913426         -1.291822   pollute\n",
       "14  0.629955              1.921778         -1.291822   tragedy\n",
       "15 -1.206206              0.085616         -1.291822   divorce\n",
       "16  1.361823              2.653645         -1.291822      jail\n",
       "17  0.690784              1.982606         -1.291822   poverty\n",
       "18 -0.112307              1.179516         -1.291822      ugly\n",
       "19 -0.785982              0.505841         -1.291822    cancer\n",
       "20  0.139679              1.431501         -1.291822      kill\n",
       "21 -0.302378              0.989444         -1.291822    rotten\n",
       "22 -0.780099              0.511723         -1.291822     vomit\n",
       "23  1.388967              2.680789         -1.291822     agony\n",
       "24  0.491985              1.783807         -1.291822    prison"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([\n",
    "pd.DataFrame([bias_score(\"GGG people are XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in unpleasant_words]),\n",
    "pd.DataFrame([bias_score(\"the GGG person is XXX.\", \n",
    "                         [[\"black\"], [\"white\"]], w) for w in unpleasant_words]),\n",
    "])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8864049736039777"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_effect_size(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_mc_perm_test(df1[\"bias_prior_corrected\"], df2[\"bias_prior_corrected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
